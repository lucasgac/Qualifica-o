\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6.5in, 9.5in}]{geometry}
\usepackage[portuguese]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{babel,arrows,positioning,chains,matrix,scopes,cd,quotes,calc,decorations.pathmorphing}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithmic}
\usepackage[portuguese, ruled, lined]{algorithm2e}
\usepackage{setspace}
\usepackage[T1]{fontenc}
\usepackage{csquotes}
\usepackage{pythonhighlight}
\usepackage{algorithmic}
\usepackage[portuguese, ruled, lined]{algorithm2e}

%quiver
\tikzset{curve/.style={settings={#1},to path={(\tikztostart)
    .. controls ($(\tikztostart)!\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    and ($(\tikztostart)!1-\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    .. (\tikztotarget)\tikztonodes}},
    settings/.code={\tikzset{quiver/.cd,#1}
        \def\pv##1{\pgfkeysvalueof{/tikz/quiver/##1}}},
    quiver/.cd,pos/.initial=0.35,height/.initial=0}

% TikZ arrowhead/tail styles.
\tikzset{tail reversed/.code={\pgfsetarrowsstart{tikzcd to}}}
\tikzset{2tail/.code={\pgfsetarrowsstart{Implies[reversed]}}}
\tikzset{2tail reversed/.code={\pgfsetarrowsstart{Implies}}}
% TikZ arrow styles.
\tikzset{no body/.style={/tikz/dash pattern=on 0 off 1mm}}

\newtheorem{definition}{Definição}
\newtheorem{proposition}[definition]{Proposição}
\newtheorem{lemma}[definition]{Lema}
\newtheorem{axiom}[definition]{Axioma}
\newtheorem{corollary}[definition]{Corolário}
\newtheorem{theorem}[definition]{Teorema}
\newtheorem{distribution}[definition]{Distribuição}
\newtheorem{example}[definition]{Exemplo}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}

\DeclareMathOperator{\freeab}{Free_{Ab}}
\DeclareMathOperator{\Top}{Top}
\DeclareMathOperator{\Ab}{Ab}
\DeclareMathOperator{\Grp}{Grp}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\module}{Mod}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\chain}{Ch}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Alt}{Alt}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\Ricci}{Ricci}
\DeclareMathOperator{\arccosh}{arccosh}
\DeclareMathOperator{\gyr}{gyr}
\DeclareMathOperator{\arctanh}{arctanh}
\DeclareMathOperator{\Log}{Log}
\DeclareMathOperator{\ArithmeticMod}{mod}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\tr}{tr}

\renewcommand{\phi}{\varphi}

\newcommand{\modulus}[1]{\phantom{.}(\ArithmeticMod\phantom{.}#1)}

\newcommand{\Mod}[1]{$\module_{#1}$}
\newcommand{\Chain}[1]{$\chain(#1)$}

\newcommand{\openset}[0]{{\phantom{}\subset}{\circ}\phantom{.}}

\def\arrvline{\hfil\kern\arraycolsep\vline\kern-\arraycolsep\hfilneg}

\title{Qualificação de Mestrado}
\author{Lucas Giraldi Almeida Coimbra}

\begin{document}

\maketitle
\tableofcontents

\section{Álgebra Linear}

\subsection{Fundamentos e Dualidade}

\subsubsection*{Fundamentos}

Tome $\mathbb{K} = \mathbb{R}$ ou $\mathbb{C}$. Um \textbf{espaço vetorial} é um conjunto $V$ munido de duas operações \begin{equation}
    \begin{split}
        + \colon V \times V &\to V \\ (x,y) &\mapsto x + y
    \end{split} \quad \text{e} \quad \begin{split}
        \cdot \colon \mathbb{K} \times V &\to V \\ (\lambda,x) &\mapsto \lambda x
    \end{split}
\end{equation} tais que a operação $+$ (soma) é comutativa, associativa, possui identidade e todos os inversos, e a operação de $\cdot$ (produto por escalar) satisfaz as relações distributivas, $1x = x$ e $\lambda(\mu x) = (\lambda\mu)x$.

Um \textbf{subespaço vetorial} de um espaço vetorial $V$ é um subconjunto $S \subset V$ tal que para todos $x, y \in S$ e $\lambda \in \mathbb{K}$, temos $x + y \in S$ e $\lambda x \in S$. Todo espaço vetorial é um subespaço vetorial de si mesmo, assim como $\{0\}$ é sempre um subespaço vetorial. Chamamos $V$ e $\{0\}$ de \textbf{subespaços triviais}. Se $U, W \subset V$ são dois subespaços, então o conjunto $U + V = \{u + w \mid u \in U, w \in W\}$ é a \textbf{soma} desses subespaços, e é também um subespaço. Se $U \cap W = \{0\}$, então diremos que a soma desses espaços é uma \textbf{soma direta} e a denotamos por $U \oplus W$. A intersecção $U \cap W$ também é sempre um subespaço vetorial.

Uma \textbf{combinação linear} de vetores em $V$ é uma soma finita \begin{equation}
    \sum_{i = 1}^n \lambda^i x_i = 0
\end{equation} onde cada $\lambda^i \in \mathbb{K}$ e cada $x_i \in V$. Dado um conjunto $S \subset V$, o conjunto $\langle S \rangle$ de todas as combinações lineares de elementos de $S$ é um subespaço vetorial de $V$, chamado de \textbf{subespaço gerado por $S$}. O conjunto $S$ é \textbf{gerador} de $V$ se $\langle S \rangle = V$.

Dizemos que $x_1, \dots, x_n \in V$ são \textbf{linearmente independentes} se para quaisquer $\lambda^1, \dots, \lambda^n \in \mathbb{K}$ tais que \begin{equation}
    \sum_{i = 1}^n \lambda^i x_i = 0,
\end{equation} então $\lambda_i = 0$ para todo $i$. Vetores que não são linearmente independentes são \textbf{linearmente dependentes}. Fica claro da definição que um conjunto de vetores é linearmente dependente se, e somente se, um dos vetores pode ser escrito como combinação linear dos outros. Além disso, é fácil ver que se uma subcoleção de vetores é linearmente dependente, então a coleção original também é. Mais ainda, qualquer coleção de vetores que contenha o $0$ é linearmente dependente.

\begin{lemma}\label{lemma1}
    Sejam $S = \{s_1, \dots, s_n\}$ um gerador de $V$ e $v_1, \dots, v_m$ vetores linearmente independentes. Então, $m \leq n$.
\end{lemma}
\begin{proof}
    Suponha que $m > n$. Como $S$ gera $V$, então existem $\lambda^1, \dots, \lambda^n$ tais que \begin{equation}
        y_1 = \sum_{i = 1}^n \lambda^i s_i.
    \end{equation} Como $y_1 \neq 0$ (pela independência linear), então algum $\lambda_j$ é não nulo, ou seja, podemos substituir $s_j$ por $y_1$ e o conjunto resultante ainda gera $V$. Pela independência linear dos $y_i$, podemos fazer essa operação mais $n-1$ vezes, garantindo que $y_1, \dots, y_n$ geram $V$. Porém, isso significa que $y_{n+1}, \dots, y_m$ são combinação linear de $y_1, \dots, y_n$, o que contradiz a independência linear. Segue então que $m \leq n$.
\end{proof}

Um espaço $V$ é \textbf{finitamente gerado} se existe um conjunto gerador finito. Uma \textbf{base} de $V$ é um conjunto gerador linearmente independente.

\begin{lemma}\label{lemma2}
    Todo espaço finitamente gerado possui uma base.
\end{lemma}
\begin{proof}
    Se $S = \{s_1, \dots, s_n\}$ gera $V$, então se $S$ é linearmente independente o trabalho acabou. Caso contrário, algum $s_i$ é combinação linear dos outros, e então retiramos ele e o conjunto resultante ainda gera $V$. Fazemos isso até que os vetores que sobram em $S$ sejam linearmente independentes, e assim temos uma base.
\end{proof}

A partir de agora vamos trabalhar apenas com espaços finitamente gerados e, caso queiramos falar em um contexto mais geral, iremos explicitar. A \textbf{dimensão} de um espaço $V$, denotada por $\dim V$, é o número de elementos de uma base. Pelo Teorema a seguir, esse número está bem definido.

\begin{theorem}
    Toda base possui mesmo número de elementos.
\end{theorem}
\begin{proof}
    Como bases são linearmente independentes e geradoras, o resultado segue facilmente do Lema \ref{lemma1}.
\end{proof}

O Lema \ref{lemma2} assume implicitamente que o conjunto $S$ que gera $V$ é não vazio. Caso tenhamos $V = \langle\varnothing\rangle$, então $V = \{0\}$ e o chamamos de \textbf{espaço trivial}. Sua dimensão é, por definição, nula.

\begin{theorem}
    Todo conjunto linearmente independente pode ser estendido para uma base.
\end{theorem}
\begin{proof}
    Se $S$ é um conjunto linearmente independente, então considere $\langle S \rangle$. Se $\langle S \rangle = V$, então o conjunto $S$ já é uma base. Caso contrário, seja $v \in V \setminus \langle S \rangle$ e tome $S_1 = S \cup \{v\}$. Podemos agora testar se $\langle S_1 \rangle = V$ e, caso contrário, repetir o processo. Como o espaço $V$ é finitamente gerado, esse processo obrigatoriamente acaba, que é quando adicionamos vetores o suficiente em $S$ para que se torne uma base.
\end{proof}

Note que todo subespaço de um espaço com dimensão finita, possui dimensão finita (pelo Lema \ref{lemma1}). Se $W$ é um subespaço de $V$, um subespaço $U$ de $V$ é um \textbf{complemento} de $W$ se $U \oplus W = V$.

\begin{theorem}
    Complementos sempre existem e são únicos.
\end{theorem}
\begin{proof}
    Se $W$ é um subespaço, seja $v_1, \dots, v_m$ uma base de $W$ e a complete para uma base $v_1, \dots, v_m, \\w_1, \dots, w_n$ de $V$. Defina $U = \langle w_1, \dots, w_n \rangle$. Se $x \in U \cap W$, então existem $\lambda^1, \dots, \lambda^m$ e $\mu^1, \dots, \mu^n$ em $\mathbb{K}$ tais que \begin{equation}
        x = \sum_{i = 1}^m \lambda^i v_i = \sum_{j = 1}^n \mu^j w_j,
    \end{equation}
    ou seja, \begin{equation}
        \sum_{i = 1}^m \lambda^i v_i + \sum_{j = 1}^n \mu^j w_j = 0,
    \end{equation} portanto cada $\lambda^i$ e $\mu^j$ é nulo, da onde segue que $x = 0$ e assim $U \cap W = \{0\}$. Mais ainda, se $x \in V$, então podemos escrever \begin{equation}
        x = \sum_{i = 1}^m \lambda^i v_i + \sum_{j = 1}^n \mu^j w_j
    \end{equation} e assim $x = v + w$ com $v \in U$ e $w \in W$, da onde segue que $V = U \oplus W$.
\end{proof}

Note que da demonstração acima tiramos um outro fato importante: se $V = U \oplus W$, então $\dim V = \dim U + \dim W$. Esse fato pode ser generalizado, isso é, se $V = V_1 + \cdots + V_n$ e $V_i \cap V_j = \{0\}$ quando $i \neq j$, então escrevemos \begin{equation}
    V = V_1 \oplus \cdots \oplus V_n = \bigoplus_{i = 1}^n V_i
\end{equation} e nesse caso temos \begin{equation}
    \dim V = \sum_{i = 1}^n \dim V_i.
\end{equation}

\begin{proposition}
    Se $V = V_1 \oplus \cdots \oplus V_n$ e $x \in V$, então existem $x_i \in V_i$ únicos tais que $x = x_1 + \cdots + x_n$.
\end{proposition}
\begin{proof}
    Considere bases $v_i^j$ de cada $V_j$, e denota $m_j = \dim V_j$. Dado $x \in V$, existe uma combinação linear \begin{equation}
        x = \sum_{i_1 = 1}^{m_1} \lambda^{i_1}_1 v_{i_1}^1 + \cdots + \sum_{i_n = 1}^{m_n} \lambda^{i_n}_n v_{i_n}^n,
    \end{equation} e portanto podemos tomar \begin{equation}
        x_j = \sum_{i_j = 1}^{m_j} \lambda^{i_j}_j v_{i_j}^j.
    \end{equation} Para mostrarmos que essa é a única maneira de decompor $x$, suponha que $x = y_1 + \cdots + y_n$ com $y_j \in V_j$. Temos que \begin{equation}
        y_j = \sum_{i_j = 1}^{m_j} \mu_j^{i_j} v_{i_j}^j
    \end{equation} e assim \begin{equation}
        \sum_{i_1 = 1}^{m_1} (\lambda^{i_1}_1 - \mu^{i_1}_1) v_{i_1}^1 + \cdots + \sum_{i_n = 1}^{m_n} (\lambda^{i_n}_n - \mu^{i_n}_n) v_{i_n}^n = 0
    \end{equation} e portanto $\lambda^{i_j}_j - \mu^{i_j}_n = 0$ para todo $j$ e todo $i_j \leq m_j$, da onde segue que $x_j = y_j$.    
\end{proof}

A \textbf{codimensão} de um subespaço $S$ em um espaço $V$ é definida por $\codim S = \dim V - \dim S$. Um espaço de codimensão $1$ é um \textbf{hiperplano} em $V$.

Uma \textbf{transformação linear} é um mapa $T \colon V \to W$ entre espaços vetoriais tal que $T(x + y) = T(x) + T(y)$ e $T(\lambda x) = \lambda T(x)$ para todos $x, y \in V$ e $\lambda \in \mathbb{K}$. Se $T$ for bijetora, dizemos que é um \textbf{isomorfismo linear} e que $V$ e $W$ são \textbf{isomorfos}. A cada transformação linear estão associados dois subespaços vetoriais: o \textbf{núcleo} e a \textbf{imagem}: \begin{equation}
    \ker T = \{v \in V \mid T(v) = 0\} \quad \text{e} \quad \im T = \{w \in W \mid w = T(v) \text{ para algum } v \in V\}.
\end{equation} É importante notar que uma transformação linear pode ser unicamente determinada pelo seus valores em alguma base do domínio. De fato, se $e_1, \dots, e_n$ é uma base de $V$, então dado $v \in V$ temos $v = \lambda^i e_i$ e portanto, por linearidade, $T(v) = \lambda^i T(e_i)$, assim basta sabermos as coordenadas de $v$ e os valores de $T$ na base para determinar $T(v)$. A partir de agora, será comum denotarmos $Tv$ para $T(v)$ caso $T$ seja linear.

\begin{proposition}
    Uma transformação linear $T \colon V \to W$ é injetora se, e somente se, $\ker T = \{0\}$. Além disso, transformações lineares preservam dependência linear, e transformações lineares injetoras preservam independência linear.
\end{proposition}
\begin{proof}
    Se $T$ é injetora, então $\ker T = \{0\}$ pois só existe um vetor que é levado em $0 \in W$, que é $0 \in V$. Agora, se $\ker T = \{0\}$, então se $T(v) = T(w)$, temos $T(v) - T(w) = 0$ e assim $T(v - w) = 0$, portanto $v - w = 0$ e assim $v = w$.

    Se $v_1, \dots, v_n$ são linearmente dependentes, então existem $\lambda^1, \dots, \lambda^n$ não toodos nulos e tais que \begin{equation}
        \sum_{i = 1}^{n} \lambda^i v_i = 0.
    \end{equation} Dessa forma, se $T$ é linear, como $T(0) = 0$ temos \begin{equation}
        \sum_{i = 1}^{n} \lambda^i T(v_i) = 0,
    \end{equation} assim os vetores $T(v_i)$ são linearmente dependentes.

    Se $v_1, \dots, v_n$ são linearmente independentes e $T$ é injetora, então considere uma combinação linear nula \begin{equation}
        \sum_{i = 1}^n \lambda^i T(v_i) = 0.
    \end{equation} Como $T$ é linear, isso equivale a dizer que \begin{equation}
        T\left(\sum_{i = 1}^n \lambda^i v_i\right) = 0
    \end{equation} e, como $T$ é injetora, então \begin{equation}
        \sum_{i = 1}^{n} \lambda^i v_i = 0,
    \end{equation} assim cada $\lambda^i = 0$ e portanto os vetores $T(v_i)$ são linearmente independentes.
\end{proof}

\begin{corollary}
    Se $V$ e $W$ tem dimensão finita, então são isomorfos se, e somente se, tem a mesma dimensão.
\end{corollary}
\begin{proof}
    Se $T \colon V \to W$ é isomorfismo, considere uma base $v_1, \dots, v_n$ de $V$. Então $T(v_1), \dots, T(v_n)$ são linearmente independentes e geram a imagem, afinal, se $w \in \im T$, então existe $v \in V$ com $T(v) = w$, assim \begin{equation}
        w = T\left(\sum_{i = 1}^n \lambda^i v_i\right) = \sum_{i = 1}^n \lambda^i T(v_i).
    \end{equation} Como $T$ é sobrejetor, $\im T = W$, portanto $T(v_1), \dots, T(v_n)$ formam base de $W$, assim $\dim V = \dim W$.

    Se $\dim V = \dim W = n$, então sejam $v_1, \dots, v_n$ e $w_1, \dots, w_n$ bases de $V$ e $W$, respectivamente. Definimos $T \colon V \to W$ por $T(v_i) = w_i$ e o estendemos por linearidade, ou seja, se \begin{equation}
        v = \sum_{i = 1}^n \lambda^i v_i,
    \end{equation} definimos \begin{equation}
        T(v) = \sum_{i = 1}^n \lambda^i w_i.
    \end{equation} Esse mapa é injetor pois se $T(v) = 0$, $\lambda^i = 0$ e assim $v = 0$. O mapa é sobrejetor pois se \begin{equation}
        w = \sum_{i = 1}^n \mu^i w_i \in W,
    \end{equation} então \begin{equation}
        T\left(\sum_{i = 1}^n \mu^i v_i\right) = w.
    \end{equation}
\end{proof}

\begin{proposition}
    Se $T \colon V \to W$ é um isomorfismo, então $T^{-1}$ também é.
\end{proposition}
\begin{proof}
    $T^{-1}$ é também bijetora, então basta mostrarmos sua linearidade. Se $v, w \in W$ e $\lambda \in \mathbb{K}$, então \begin{equation}
        T^{-1}(v + w) = T^{-1}(T(T^{-1}(v)) + T(T^{-1}(w))) = T^{-1}(T(T^{-1}(v) + T^{-1}(w))) = T^{-1}(v) + T^{-1}(w)
    \end{equation} e, além disso, \begin{equation}
        T^{-1}(\lambda v) = T^{-1}(\lambda T(T^{-1}(v))) = T^{-1}(T(\lambda T^{-1}(v))) = \lambda T^{-1}(v).
    \end{equation}
\end{proof}

Fixado $W$ um subespaço de um espaço $V$, podemos definir uma relação de equivalência, denotada por \begin{equation}
    u \equiv v \mod W
\end{equation} se, e somente se, $u - v \in W$. Denotamos a classe de equivalência de $v \in V$ por $[v]$ e o conjunto de todas as classes de equivalência por $V/W$, que será chamado de \textbf{quociente de V por W}. Esse conjunto possui estrutura de espaço vetorial utilizando as seguintes operações, que estão bem definidas: \begin{equation}
    [v] + [w] = [v + w] \quad \text{e} \quad \lambda [v] = [\lambda v].
\end{equation}

\begin{proposition}
    Se $V$ é um espaço vetorial e $W$ um subespaço, então $\dim V/W = \dim V - \dim W$. Mais precisamente, se $w_1, \dots, w_n$ é uma base de $V$ de maneira que $w_1, \dots, w_n, v_1, \dots, v_m$ é uma base de $V$, então $[v_1], \dots, [v_m]$ é uma base de $V/W$.
\end{proposition}
\begin{proof}
    Primeiro, vamos verificar que $[v_1], \dots, [v_m]$ são linearmente independentes. De fato, considere uma combinação linear nula \begin{equation}
        \sum_{i = 1}^n \lambda^i [v_i] = [0].
    \end{equation} Pela definição das operaçções no quociente, temos que \begin{equation}
        \left[\sum_{i = 1}^n \lambda^i v_i\right] = [0],
    \end{equation} ou seja, \begin{equation}
        \sum_{i = 1}^m \lambda^i v_i = w \in W.
    \end{equation} Escrevendo $w$ na base de $W$, temos \begin{equation}
        \sum_{i = 1}^m \lambda^i v_i = \sum_{j = 1}^n \mu^j w_j,
    \end{equation} portanto $\lambda^i = \mu^j = 0$, da onde segue que $[v_1], \dots, [v_m]$ são linearmente independentes. O próximo passo é mostrar que esses vetores geram $V/W$. Se $[v] \in V/W$, então \begin{equation}
        v = \sum_{i = 1}^m \lambda^i v_i + \sum_{j = 1}^n \mu^j w_j
    \end{equation} e assim \begin{equation}
        [v] = \left[\sum_{i = 1}^m \lambda^i v_i + \sum_{j = 1}^n \mu^j w_j\right] = \sum_{i = 1}^m \lambda^i [v_i] + \sum_{j = 1}^n \mu^j [w_j] = \sum_{i = 1}^m \lambda^i [v_i] + \sum_{j = 1}^n \mu^j [0] = \sum_{i = 1}^m \lambda^i [v_i].
    \end{equation}
\end{proof}

Como corolário, se $\dim V = \dim W$, temos que $\dim V/W = 0$, portanto $V/W = \{0\}$ e assim $V = W$. O proxímo item é o que chamamos de teorema do isomorfismo, na sua versão linear.

\begin{theorem}
    Se $T \colon V \to W$ é linear, então o mapa \begin{equation}
        \begin{split}
            \tilde{T} \colon V/\ker T &\to \im T \\ [v] &\mapsto T(v)
        \end{split}
    \end{equation} está bem definido e é um isomorfismo linear. Como consequência, temos o teorema do núcleo e imagem: \begin{equation*}
        \dim V = \dim \im T + \dim \ker T.
    \end{equation*}
\end{theorem}
\begin{proof}
    O mapa é sobrejetor, afinal, se $w \in \im T$, então existe $v \in V$ tal que $T(v) = w$, portanto $\tilde{T}([v]) = w$. O mapa é injetor, afinal, se $[v] \in \ker \tilde{T}$, então $\tilde{T}([v]) = 0$, assim $T(v) = 0$, portanto $v \in \ker T$ e assim $[v] = [0]$. Como $\dim V/\ker T = \dim V - \dim \ker T$ e $\tilde{T}$ é isomorfismo, então \begin{equation}
        \dim V - \dim \ker T = \dim \im T,
    \end{equation} de onde segue o resultado.
\end{proof}

Se $V$ e $W$ são espaços vetoriais, o conjunto $V \times W$ é também um espaço vetorial com as operações \begin{equation}
    (x,y) + (v,w) = (x+v, y+w) \quad \text{e} \quad \lambda(x,y) = (\lambda x, \lambda y).
\end{equation}

Se $V = U + W$, e $I = U \cap W$, então podemos tomar $T \colon U \times W \to V$ dada por $(u,w) \mapsto u+w$. Como $V = U + W$, o mapa é sobrejetor. Além disso, seu núcleo é $\{(x,-x) \mid x \in I\}$, que é isomorfo a $I$, assim segue do teorema do isomorfismo que \begin{equation}
    \dim U \times V = \dim V + \dim U \cap W
\end{equation} e, como $\dim U \times V = \dim U + \dim V$, segue que \begin{equation}
    \dim V = \dim U + \dim W - \dim U \cap W.
\end{equation}

\begin{proposition}\label{prop12}
    Se $V$ e $W$ possuem a mesma dimensão finita, então são equivalentes as seguintes afirmações sobre uma transformação linear $T \colon V \to W$: \begin{itemize}
        \item $T$ é injetora;
        \item $T$ é sobrejetora;
        \item $T$ é um isomorfismo;
        \item $T$ leva bases em bases;
    \end{itemize}
\end{proposition}
\begin{proof}
    Se $T$ é injetora, então $\ker T = \{0\}$ e assim, pelo teorema do núcleo e imagem, $\dim W = \dim V = \dim \ker T + \dim \im T = \dim \im T$, assim $\im T = W$ e $T$ é sobrejetora.

    Se $T$ é sobrejetora, então $\dim \im T = \dim W = \dim V$, assim $\dim V = \dim \ker T + \dim V$, portanto $\dim \ker T = 0$ e portanto $\ker T = \{0\}$, da onde segue que $T$ é injetora, e portanto um isomorfismo (pois já é sobrejetora).

    Se $T$ é isomorfismo, então se $v_1, \dots, v_n$ é base de $V$, então pela injetividade, $T(v_1), \dots, T(v_n)$ são linearmente independentes. Mais ainda, $T(v_1), \dots, T(v_n)$ geram a imagem de $T$, que é $W$, portanto esses vetores formam base de $W$.

    Por fim, se $T$ leva bases em bases, então se $v_1, \dots, v_n$ é base de $V$ e $T(v) = 0$, então se \begin{equation}
        v = \sum_{i = 1}^n \lambda^i v_i,
    \end{equation} temos \begin{equation}
        0 = T(v) = \sum_{i = 1}^n \lambda^i T(v_i),
    \end{equation} da onde segue, pela independência linear de $T(v_1), \dots, T(v_n)$, que $\lambda^i = 0$, assim $v = 0$ e portanto $T$ é injetora.
\end{proof}

\subsubsection*{Dualidade}

A partir de agora, todo espaço será finitamente gerado, ou em outros termos, terá dimensão finita. Seja $V$ um espaço vetorial sobre $\mathbb{K}$ (lembrando que $\mathbb{K} = \mathbb{R}, \mathbb{C}$). Um \textbf{funcional} ou \textbf{covetor} em $V$ é um mapa linear $\phi \colon V \to \mathbb{K}$. Denotamos o conjunto de todos os funcionais lineares em $V$ por $V^*$. Esse conjunto se torna um espaço vetorial ao definirmos \begin{equation}
    (\phi + \psi)(v) = \phi(v) + \psi(v) \quad \text{e} \quad (\lambda \phi)(v) = \lambda \phi(v).
\end{equation}

Fixada uma base $e = (e_1, \dots, e_n)$ de $V$ (aqui, $e$ representa uma lista de $n$ vetores), se $v[e] = (v^1, \dots, v^n)$ são as coordenadas de $v$ na base $e$, isso é, \begin{equation}
    v = \sum_{i = 1}^n v^i e_i
\end{equation}, então podemos definir os mapas \begin{equation}
    \begin{split}
        \varepsilon^i \colon V &\to \mathbb{K} \\ v &\mapsto v^i
    \end{split},
\end{equation} que vamos chamar de \textbf{diferenciais} com respeito a base $e$. A lista de vetores $\varepsilon = (\varepsilon^1, \dots, \varepsilon^n)$ é uma base de $V^*$, chamada de \textbf{base dual} de $e$. O fato dessa lista ser uma base implica diretamente que o mapa $e_i \mapsto \varepsilon^i$ é um isomorfismo entre $V$ e $V^*$. Esse isomorfismo não é natural, isso é, ele depende da escolha de base $e$. Uma outra base gera outro isomorfismo, e não existe uma base canônica que podemos considerar.

Para um exemplo de isomorfismo natural, podemos considerar o \textbf{espaço bidual} de $V$, que é simplesmente $V^{**}$, isso é, o conjunto de todos os funcionais em $V^*$. Se $v \in V$ e $\phi \in V^*$, podemos definir $v(\phi) = \phi(v)$ e portanto tratar cada $v \in V$ como um elemento de $V^{**}$. Essa identificação gera um isomorfismo entre $V$ e $V^{**}$ que não depende de nenhuma escolha arbitrária.

Seja $W$ um subespaço de $V$. O \textbf{aniquilador} de $W$, denotado por $W^\perp$, é o subespaço de $V^*$ consistido de todos os covetores que se anulam em $W$.

\begin{proposition}
    Se $W$ é subespaço de $V$, então \begin{equation}
        \dim W^\perp + \dim W = \dim V,
    \end{equation} ou seja, $\dim W^\perp = \codim W$.
\end{proposition}
\begin{proof}
    Considere uma base $w_1, \dots, w_n$ de $W$ e um completamento $w_1, \dots, w_n, v_1, \dots, v_m$ para uma base de $V$. Defina $T \colon V \to V^*$ por $T(w_i) = 0$ e $T(v_i) = \nu_i$, onde $\nu_i$ são os elementos da base dual correspondentes a $v_i$. O núcleo de $T$ é claramente $W$, basta mostrarmos que $\im T = W^\perp$.

    Se $\phi \in W^\perp$, então \begin{equation}
        \phi = \sum_{i = 1}^n \lambda^i \omega_i + \sum_{j = 1}^m \mu^j \nu_j.
    \end{equation} e assimm $\phi(w_i) == \lambda^i$, mas $\phi \in W^\perp$, portanto $\phi(w_i) = 0$, assim \begin{equation}
        \phi = \sum_{j = 1}^m \mu^j \nu_j
    \end{equation} e portanto \begin{equation}
        T\left(\sum_{j = 1}^m \mu^j v_j\right) = \phi,
    \end{equation} assim $\phi \in \im T$. Por outro lado, se $\phi \in \im T$, então $\phi$ é combinação linear dos covetores $\nu_1, \dots, \nu_m$, que estão todos em $W^\perp$, portanto $\phi \in W^\perp$, assim $\im T = W^\perp$.
\end{proof}

\begin{proposition}
    Se $W$ é um subespaço de $V$, então o isomorfismo $v \mapsto (\phi \mapsto \phi(v))$ identifica $W$ com $W^{\perp \perp}$.
\end{proposition}
\begin{proof}
    De fato, se $v \in W$, então precisamos mostrar que $v(\phi) = 0$ para todo $\phi \in W^\perp$, isso é, $w \in W^{\perp\perp}$. Isso, porém, é óbvio, já que $v(\phi) = \phi(v) = 0$, já que $v \in W$.

    Agora, se $w \in W^{\perp\perp}$, então $w(\phi) = 0$ para todo $\phi \in W^\perp$, ou seja, $\phi(w) = 0$ para todo $\phi \in W^\perp$, assim $w \in W$.
\end{proof}

\subsection{Mapas Lineares, Matrizes, Determinante e Traço}

\subsubsection*{Mapas Lineares e Matrizes}

Um fato utilizando anteriormente, e que não foi provado, é que a composição de mapas lineares é linear.

\begin{proposition}
    Composição de mapas lineares é linear e, em particular, composição de isomorfismos é isomorfismo. Mais ainda, se $S$ e $T$ são isomorfismos, $(S \circ T)^{-1} = T^{-1} \circ S^{-1}$.
\end{proposition}
\begin{proof}
    Se $T \colon V \to W$ e $S \colon W \to U$ são lineares, então dados $u,v \in V$ e $\lambda \in \mathbb{K}$, temos \begin{equation}
        (S \circ T)(u + v) = S(T(u + v)) = S(T(u) + T(v)) = S(T(u)) + S(T(v)) = (S \circ T)(u) + (S \circ T)(v)
    \end{equation} e, além disso, \begin{equation}
        (S \circ T)(\lambda u) = S(T(\lambda u)) = S(\lambda T(u)) = \lambda S(T(u)) = \lambda (S \circ T)(u).
    \end{equation}

    Se $T$ e $S$ forem isomorfismos, então \begin{equation}
        (S \circ T) \circ (T^{-1} \circ S^{-1}) = S \circ (T \circ T^{-1}) \circ S^{-1} = S \circ S^{-1} = \Id_U
    \end{equation} e, além disso, \begin{equation}
        (T^{-1} \circ S^{-1}) \circ (S \circ T) = T^{-1} \circ (S^{-1} \circ S) \circ T = T^{-1} \circ T = \Id_V.
    \end{equation}
\end{proof}

Além disso, a demonstração de que o núcleo de um mapa linear é um subespaço vetorial também nunca foi apresentada, mas isso é por que esse fato é um corolário de um resultado um pouco mais geral.

\begin{proposition}
    Se $T \colon V \to W$ é linear e $U \subset W$ é um subespaço, então $T^{-1}(U)$ é um subesaço
\end{proposition}
\begin{proof}
    De fato, se $u, v \in T^{-1}(U)$, então $T(u), T(v) \in W$, assim $T(u + v) = T(u) + T(v) \in W$, portanto $u + v \in T^{-1}(W)$. Mais ainda, se $\lambda \in \mathbb{K}$, então $T(\lambda u) = \lambda T(u) \in W$, assim $\lambda u \in T^{-1}(W)$.
\end{proof}

Podemos observar que, em coordenadas, todo mapa linear possui uma forma canônica.

\begin{proposition}\label{prop17}
    Se $\dim V = n$, $\dim W = m$ e $T \colon V \to W$ é linear, então fixadas $e$ e $f$ bases de $V$ e $W$ temos \begin{equation}
        Tv[f] = \left(\sum_{i_1 = 1}^n \lambda^1_{i_1} v[e]^{i_1}, \dots, \sum_{i_m = 1}^n \lambda^m_{i_m} v[e]^{i_m}\right)
    \end{equation} onde $Tv[f]$ são as coordenadas de $Tv$ na base $f$ e $v[e]^j$ é a $j$-ésima coordenada de $v$ na base $e$.
\end{proposition}

Podemos organizar os números $\lambda^i_j$ da Proposição \ref{prop17} em um retângulo da forma \begin{equation}
    \begin{bmatrix}
        \lambda^1_1 & \lambda^1_2 & \cdots & \lambda^1_{n-1} & \lambda^1_n \\ \lambda^2_1 & \lambda^2_2 & \cdots & \lambda^2_{n-1} & \lambda^2_n \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ \lambda^{m-1}_1 & \lambda^{m-1}_2 & \cdots & \lambda^{m-1}_{n-1} & \lambda^{m-1}_n \\  \lambda^m_1 & \lambda^m_2 & \cdots & \lambda^m_{n-1} & \lambda^m_n
    \end{bmatrix}
\end{equation}

Uma tabela nesse estilo será chamada de \textbf{matriz}. Cada coleção de números colocados na horizontal será uma \textbf{linha} da matriz, e cada coleção de números na vertical será uma \textbf{coluna} da matriz. Perceba que toda transformação da origem a uma matriz como a descrita acima, que vamos chamar de \textbf{matriz de $T$ com respeito as bases $e$ e $f$}. Para abreviar a notação, denotamos a matriz acima por $[T]_{e, f} = [\lambda^i_j]_{m \times n}$, onde $m$ e $n$ são o número de linhas e colunas, respectivamente.

Agora vamos introduzir as operações em matrizes. Se $A = [a^i_j]_{n \times m}$ e $B = [b^i_j]_{n \times m}$ são matrizes, então podemos somá-las e multiplicar uma delas por um escalar da seguinte forma: \begin{equation}
    A + B = [a^i_j + b^i_j]_{n \times m} \quad \text{e} \quad \lambda A = [\lambda a^i_j]_{n \times m}.
\end{equation} Além disso, se $C = [c^i_j]_{m \times k}$ então podemos definir um produto entre $A$ e $C$ (note que o número de linhas de $C$ deve ser o mesmo número de colunas de $A$) fazendo \begin{equation}
    AC = \left[\sum_{s = 1}^m a^i_s c^s_j\right]_{n \times k}.
\end{equation} O produto não é comutativo, afinal, se $k \neq n$ então $CA$ pode nem estar definido. Se $k = n \neq m$, então necessariamente $AC \neq CA$, visto que $AC$ é $n \times n$ e $CA = m \times m$. Por fim, mesmo que $n = m = k$, poderíamos ter $AC \neq CA$, por exemplo: \begin{equation}
    \begin{bmatrix}
        2 & 1 \\ 0 & 1
    \end{bmatrix} \begin{bmatrix}
        3 & -2 \\ 1 & 3
    \end{bmatrix} = \begin{bmatrix}
        7 & -1 \\ 1 & 3
    \end{bmatrix} \quad \text{e} \quad \begin{bmatrix}
        3 & -2 \\ 1 & 3
    \end{bmatrix} \begin{bmatrix}
        2 & 1 \\ 0 & 1
    \end{bmatrix} = \begin{bmatrix}
        6 & 1 \\ 2 & 4
    \end{bmatrix}.
\end{equation}

\begin{proposition}
    Toda matriz vem de um mapa linear.
\end{proposition}
\begin{proof}
    Se $A = [a^i_j]$ é uma matriz $n \times m$, podemos enxergá-la como um mapa $\mathbb{R}^m \to \mathbb{R}^n$ que pega vetores $x = (x^1, \dots, x^n)$ e retorna $Ax$, onde $x$ é visto como a matriz coluna \begin{equation}
        x = \begin{bmatrix}
            x^1 \\ \vdots \\ x^n
        \end{bmatrix}
    \end{equation} e $Ax$, que também é uma matriz coluna $n \times 1$, é identificada com um vetor de $\mathbb{R}^n$ da mesma maneira. O mapa é linear, afinal, \begin{equation}
        A(x + y) = \left[\sum_{k = 1}^m a^i_k (x^k + y^k) \right]_{n \times 1} = \left[\sum_{k = 1}^m a^i_k x^k + \sum_{k = 1}^m a^i_k y^k \right]_{n \times 1} = Ax + Ay
    \end{equation} e, além disso, \begin{equation}
        A(\lambda x) = \left[\sum_{k = 1}^m a^i_k (\lambda x^k) \right]_{n \times 1} = \left[\lambda \sum_{k = 1}^m a^i_k x^k \right]_{n \times 1} = \lambda Ax.
    \end{equation} É fácil notar que a matriz de $A$ com respeito às bases canônicas de $\mathbb{R}^m$ e $\mathbb{R}^n$ é a própria $A$.
\end{proof}

Todo sistema linear pode ser traduzido para uma equação de matrizes. De fato, considere o sistema linear \begin{equation}
    \begin{cases}
        a^1_1 x^1 + \cdots + a^1_m x^m = b^1 \\ \quad \quad \quad \quad \quad \vdots \\ a^n_1 x^1 + \cdots + a^n_m x^m = b^n
    \end{cases}
\end{equation} e perceba que, denotando $x = (x^1, \dots, x^n)$ e $b = (b^1, \dots, b^n)$, podemos escrever esse sistema simplesmente como $Ax = b$, onde $A = [a^i_j]_{n \times m}$.

Se $n > m$, o sistema é dito \textbf{sobredeterminado}, e existem mais equações do que incógnitas. Nesse caso a matriz nunca pode ser sobrejetora, e sempre vão existir infinitos vetores $b$ tais que $Ax = b$ não possui solução. Se $b \in \im A$, note que $A^{-1}(b) = y + \ker A$ para algum $y$ tal que $Ay = b$, então ou toda solução é única (se $A$ for injetora) ou nenhuma solução é única (caso contrário).

Se $m > n$, isso é, existe mais incógnitas do que equações, então $A$ nunca é injetora, e assim sempre que existe solução para $Ax = b$, a solução nunca é única. A justificativa é simples, afinal, $\dim \ker A = m - \dim \im A$ e $\dim \im A \leq n < m$, assim $\dim \ker A > 0$ e, se $b \in \im A$, então $A^{-1}(b) = y + \ker A$ para alguma solução $y$, assim $A^{-1}(b)$ é infinito.

Se $n = m$, então se $Ax = b$ sempre tem solução, ela é sempre única, pois sobrejetividade e injetividade de $A$ são equivalentes. Nesse caso, $A$ é um isomorfismo, denotamos sua inversa por $A^{-1}$ e a solução do sistema é $x = A^{-1}b$. Note que checar a injetividade de $A$ é determinar se $Ax = 0$ possui solução não nula. Assumimos que o leitor já sabe técnicas de escalonamento de sistemas e portanto achar soluções já seja uma ferramenta conhecida.

\begin{proposition}
    Se $S, T \colon V \to U$ e $R \colon U \to W$ são lineares e $e$, $f$ e $g$ são bases de $V$, $U$ e $W$, respectivamente, então \begin{equation}
        [T + S]_{e, f} = [T]_{e, f} + [S]_{e, f}, \quad [\lambda T]_{e, f} = \lambda [T]_{e, f} \quad \text{e} \quad [R \circ T]_{e, g} = [R]_{f,g} [T]_{e,f}.
    \end{equation}
\end{proposition}
\begin{proof}
    Note que, se $[T]_{e,f} = [\lambda^i_j]$, $[S]_{e,f} = [\mu^i_j]$ e $[R]_{f,g} = [\nu^i_j]$, então \begin{equation}
        Te_j = \sum_{i = 1}^{\dim U} \lambda^i_j f_i, \quad Se_j = \sum_{i = 1}^{\dim U} \mu^i_j f_i \quad \text{e} \quad Rf_j = \sum_{i = 1}^{\dim W} \nu^i_j g_i,
    \end{equation} assim \begin{equation}
        (T + S)e_j = Te_j + Se_j = \sum_{i = 1}^{\dim U} \lambda^i_j f_i + \sum_{i = 1}^{\dim U} \mu^i_j f_i = \sum_{i = 1}^{\dim U} (\lambda^i_j + \mu^i_j) f_i,
    \end{equation} portanto o resultado da soma segue. Similarmente, \begin{equation}
        (\delta T)e_j = \delta Te_j = \delta \sum_{i = 1}^{\dim U} \lambda^i_j f_i = \sum_{i = 1}^{\dim U} (\delta \lambda^i_j) f_i,
    \end{equation} portanto o resultado do produto por escalar segue. Por fim, \begin{equation}
        (R \circ T)e_j = RTe_j = R\left(\sum_{i = 1}^{\dim U} \lambda^i_j f_i\right) = \sum_{i = 1}^{\dim U} \lambda^i_j Rf_i = \sum_{i = 1}^{\dim U} \lambda^i_j \sum_{k = 1}^{\dim W} \nu^k_i g_k = \sum_{k = 1}^{\dim W} \left(\sum_{i = 1}^{\dim U} \lambda^i_j \nu^k_i\right) g_k,
    \end{equation} da onde segue o resultado da composição.
\end{proof}

Esse resultado é tudo que precisamos para tratar matrizes e mapas lineares como as mesmas entidades. Em particular, isso garante que $(AB)^{-1} = B^{-1}A^{-1}$ para matrizes também! Mesmo que o resultado tenha sido provado apenas para mapas.

\begin{proposition}
    Se $T, S \colon V \to U$ e $R \colon U \to W$ são lineares, então vale que $R \circ (T + S) = R \circ T + R \circ S$. Se $Q \colon W \to V$ é linear, então vale que $(T + S) \circ Q = T \circ Q + S \circ Q$.
\end{proposition}
\begin{proof}
    Na primeira distributiva, basta usar que $R$ é linear e, na segunda, basta aplicar a definição da soma de mapas.
\end{proof}

Se $T \colon V \to U$ é linear, então induz um mapa $T^\top \colon U^* \to V^*$, chamado de \textbf{transposto} de $T$, dado por $T^\top(\phi) = \phi \circ T$. Ao mesmo tempo, dada uma matriz $A = [a^i_j]_{n \times m}$ definimos sua \textbf{matriz transposta} por $A^\top = [a^j_i]_{m \times n}$. Adivinhem?

\begin{proposition}
    Se $e$ é uma base de $V$, $f$ é uma base de $U$ e $\varepsilon$ e $\delta$ são suas bases duais, então \begin{equation}
        [T']_{\delta, \varepsilon} = [T]_{e, f}^\top
    \end{equation}
\end{proposition}
\begin{proof}
    Como $\dim V = \dim V^*$ e $\dim U = \dim U^*$, as dimensões das duas matrizes batem. Agora, note que se \begin{equation}
        Te_j = \sum_{i = 1}^{\dim U} \lambda^i_j f_i,
    \end{equation} então \begin{equation}
        (T^\top \delta^j)(e_k) = (\delta^j \circ T)(e_k) = \delta^j\left(\sum_{i = 1}^{\dim U} \lambda^i_k f_i\right) = \lambda^j_k,
    \end{equation} assim, se $v[e] = (v^1, \dots, v^n)$ então \begin{equation}
        (T^\top\delta^j)(v) = \sum_{i = 1}^{n} \lambda^j_i v^i = \sum_{i = 1}^n \lambda^j_i \varepsilon^i(v),
    \end{equation} portanto \begin{equation}
        T^\top \delta^j = \sum_{i = 1}^n \lambda^j_i \varepsilon^i,
    \end{equation} provando assim o resultado.
\end{proof}

Vamos agora mostrar alguns fatos sobre a transposta de mapas lineares.

\begin{proposition}
    Se $T,S \colon V \to U$ e $R \colon U \to W$ são lineares, então $(R \circ T)^\top = T^\top \circ R^\top$, $(T + S)^\top = T^\top + S^\top$ e, se $T$ for um isomorfismo, então $T^\top$ também é e $(T^\top)^{-1} = (T^{-1})^\top$.
\end{proposition}
\begin{proof}
    Note que \begin{equation}
        (R \circ T)^\top\phi = \phi \circ R \circ T = T^\top(\phi \circ R) = T^\top(R^\top \phi)
    \end{equation} e, além disso, \begin{equation}
        (T + S)^\top\phi = \phi \circ (T + S) = \phi \circ T + \phi \circ S = T^\top \phi + S^\top phi.
    \end{equation} Por fim, se $T$ for isomorfismo, então \begin{equation}
        (T^\top \circ (T^{-1})^\top)\phi = \phi \circ T^{-1} \circ T = \phi \quad \text{e} ((T^{-1})^\top \circ T^\top)\phi = \phi \circ T \circ T^{-1} = \phi,
    \end{equation} portanto $(T^\top)^{-1} = (T^{-1})^\top$.
\end{proof}

Até agora, temos maquinário para provar um monte de coisas sobre matrizes, usando mapas lineares. Mas e o contrário, é possível? Podemos garantir, por exemplo, que $T^{\top\top} = T$ (usando a idenficação natural $V^{**} = V$) usando apenas que isso é óbvio para matrizes? Sim! Porém, devemos anter terminar a nossa correspondência entre mapas lineares e matrizes. Já sabemos que toda matriz é um mapa linear, e todo mapa linear pode ser representado por uma matriz. Resta mostrar que essa correspondência ``vai e volta'': \begin{quotation}
    Se $T \colon V \to U$ é um mapa linear, então o mapa induzido por uma matriz $[T]_{e,f}$ corresponde a $T$. Se $A$ é uma matriz, então a matriz do mapa induzido por $A$, em alguma base, é $A$.
\end{quotation}
A segunda parte já concluímos anteriormente, então basta entendermos a primeira mais precisamente, e prová-la.

\begin{proposition}
    Se $T \colon V \to U$ é um mapa linear, $\dim V = n$, $\dim U = m$ e $e$ e $f$ são bases de $V$ e $U$, então o mapa $\mathbb{R}^n \to \mathbb{R}^m$ induzido por $[T]_{e,f}$ leva $v[e]$ em $Tv[f]$.
\end{proposition}
\begin{proof}
    Sejam $[T]_{e,f} = [\lambda^i_j]_{m \times n}$ e $v[e] = (v^1, \dots, v^n)$. Então o mapa induzido por $[T]_{e,f}$ é dado por \begin{equation}
        [T]_{e,f}(v^1, \dots, v^n) = \left(\sum_{i = 1}^n \lambda^1_i v^i, \dots, \sum_{i = 1}^n \lambda^m_i v^i\right).
    \end{equation} Agora, note que \begin{equation}
        Tv = T\left(\sum_{i = 1}^n v^i e_i\right) = \sum_{i = 1}^n v^i T(e_i) = \sum_{i = 1}^n v^i \sum_{j = 1}^m \lambda^j_i f_j = \sum_{j = 1}^m \left(\sum_{i = 1}^n \lambda^j_i v^i\right) f_j,
    \end{equation} o que conclui que $Tv[f] = [T]_{e,f}v[e]$.
\end{proof}

\begin{corollary}
    Se $T, S \colon V \to U$ são lineares e existem bases $e$ e $f$ de $V$ e $U$ tais que $[T]_{e,f} = [S]_{e,f}$, então $T = S$.
\end{corollary}
\begin{proof}
    Como $[T]_{e,f} = [S]_{e,f}$, então eles levam $v[e]$ em $Tv[f] = Sv[f]$, portanto $Tv = Sv$.
\end{proof}

Vamos usar isso ao nosso favor! Note que, em termos de matrizes, fica bem claro que, para qualquer matriz $T$, $T^{\top\top} = T$, já que estamos apenas trocando linhas por colunas, duas vezes. Isso indica, claro, que o mesmo vale para mapas lineares!

\begin{proposition}
    Fazendo as identificações naturais de $V^{**} = V$ e $U^{**} = U$, se $T \colon V \to U$ é linear, então $T^{\top\top} = T$.
\end{proposition}
\begin{proof}
    Como \begin{equation}
        [T]^\top_{e,f} = [T^\top]_{\delta, \varepsilon} \implies [T]^{\top \top}_{e,f} = [T^\top]^\top_{\delta, \varepsilon},
    \end{equation} e também \begin{equation}
        [T]^{\top\top}_{e,f} = [T]_{e,f} \quad \text{e} \quad [T^\top]^\top_{\delta, \varepsilon} = [T^{\top\top}]_{e,f},
    \end{equation} então \begin{equation}
        [T]_{e,f} = [T^{\top\top}]_{e,f}.
    \end{equation}
\end{proof}

Prosseguindo com matrizes, vamos agora falar de posto. Dada uma matriz $A$ de dimensões $n \times m$, o \textbf{posto} de $A$, denotado $\rank A$, e que é a dimensão da imagem de $A$. Também definimos o \textbf{posto de linhas} de $A$, que é o maior número de linhas de $A$ linearmente independentes, quando consideradas como vetores de $\mathbb{R}^m$. Por fim, definimos o \textbf{posto de colunas} de $A$, que é o maior número de colunas de $A$ linearmente independentes, quando consideradas como vetores de $\mathbb{R}^n$. A ideia é mostrarmos que todos esses são equivalentes.

\begin{proposition}
    As colunas de $A = [a^i_j]_{n \times m}$ geram $\im A$. 
\end{proposition}
\begin{proof}
    Se $x \in \mathbb{R}^m$, então $Ax \in \mathbb{R}^n$ e, se $x = (x^1, \dots, x^m)$, temos \begin{equation}
        Ax = \left(\sum_{j = 1}^m a^1_j x^j, \dots, \sum_{j = 1}^m a^n_j x^j\right) = \sum_{j = 1}^m x^j (a^1_j, \dots, a^n_j).
    \end{equation}
\end{proof}

\begin{corollary}
    O posto de colunas de uma matriz é igual ao posto dessa mesma matriz.
\end{corollary}

A parte problemática é mostrar que o posto de linhas e colunas é o mesmo. Bom, o caminho para isso é simples: o posto de linhas de $A$ é o posto de colunas de $A^\top$, que é o posto de $A^\top$. Se mostrarmos que $\rank A^\top = \rank A$, o trabalho terminou. Para isso, vamos utilizar a noção de aniquilador que vimos anteriormente.

\begin{theorem}
    Se $T \colon V \to U$ é linear, então $(\im T)^\perp = \ker T^\top$.
\end{theorem}
\begin{proof}
    Se $\phi \in (\im T)^\perp$, então dado $v \in V$, $\phi(Tv) = 0$, isso é, $T^\top\phi = 0$, ou seja, $\phi \in \ker T^\top$. Por outro lado, se $\phi \in \ker T^\top$, então $T^\top \phi = 0$, ou seja, se $u \in \im T$, então existe $v \in V$ com $u = Tv$, assim $\phi(u) = \phi(Tv) = 0$ e portanto $\phi \in (\im T)^\perp$.
\end{proof}

\begin{corollary}
    Se $T \colon V \to U$ é linear, então $\dim \im T = \dim \im T^\top$.
\end{corollary}
\begin{proof}
    Pelo teorema do núcleo e imagem, \begin{equation}
        \dim \im T^\top + \dim \ker T^\top = \dim U^*
    \end{equation} e, como \begin{equation}
        \dim (\im T)^\perp + \dim \im T = \dim U,
    \end{equation} então o resultado segue, usando que $\dim U = \dim U^*$ e $\dim (\im T)^\perp = \dim \ker T^\top$.
\end{proof}

Vamos agora focar a discussão em transformações lineares da forma $T \colon V \to V$, que costumamos chamar de \textbf{operadores} em $V$. Denotamos o conjunto dos operadores por $\mathcal{L}(V,V)$. Por consequência, nossa discussão de matrizes irá também se limitar a \textbf{matrizes quadradas}, isso é, aquelas em que o número de linhas é igual ao número de colunas. Note que a composição de mapas em $\mathcal{L}(V,V)$ sempre está definida, assim como o produto de duas matrizes quadradas de mesmas dimensões, por mais que ele não seja comutativo. O mapa identidade $\Id \colon V \to V$ é claramente linear e, dada $e$ uma base de $V$, como $\Id(e) = e$, fica claro que \begin{equation}
    [\Id]_{e,e} = \begin{bmatrix}
        1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1
    \end{bmatrix},
\end{equation} que a partir de agora será chamada de \textbf{matriz identidade $n \times n$}, em que $n = \dim V$.

A partir de agora, iremos utilizar que a composição de mapas corresponde ao produto de matrizes e omitir o sinal de composição em mapas lineares. Dados dois mapas lineares $T, S \in \mathcal{L}(V,V)$, definimos $T_S = STS^{-1}$. O mapa $T \mapsto T_S$ é uma \textbf{conjugação}. Dizemos que $T_S$ é \textbf{similar} ou \textbf{conjugada} a $T$. Esse mapa é claramente um isomorfismo linear (basta ver que a conjugação por $S^{-1}$ é o mapa inverso da conjugação por $S$).

\begin{theorem}
    Conjugação é uma relação de equivalência.
\end{theorem}
\begin{proof}
    Se $R = T_S$ para alguma $T$, então $T = R_{S^{-1}}$, assim conjugação é simétrica. Mais ainda, $T = T_{\Id}$, então conjugação é reflexiva. Por fim, se $R = T_S$ e $T = K_L$, então $R = T_S = (K_L)_S = K_{SL}$. 
\end{proof}

Mais ainda, fica claro que se $M$ é invertível e conjugada a $T$, então $T$ é invertível. De fato, se $M = T_S$, então $T = M_{S^{-1}} = M_S^{-1}$, portantoo $T^{-1} = M_S$.

\subsubsection*{Determinantes e Traço}

Para definirmos a noção de determinante, precisamos de permutações. Se $X = \{x_1, \dots, x_n\} \subset \mathbb{N}$, uma \textbf{permutação em $X$} é uma bijeção $\sigma \colon X \to X$. O \textbf{discriminante} de $X$ é o número \begin{equation}
    D(x_1, \dots, x_n) = \prod_{i < j} (x_i - x_j).
\end{equation} O \textbf{sinal} da permutação $\sigma$ é o número $(-1)^\sigma$ que satisfaz \begin{equation}
    P(x_1, \dots, x_n) = (-1)^\sigma P(X_{\sigma(1)}, \dots, x_{\sigma(n)}).
\end{equation} Denotamos por $S_n$ o conjunto de todas as permutações em $[n] = \{1, \dots, n\}$. Se $A = [a^i_j]_{n \times n}$ é uma matriz, definimos o determinante \begin{equation}
    \det(A) = \sum_{\sigma \in S_n} (-1)^\sigma a^1_{\sigma(1)} \cdots a^n_{\sigma(n)}.
\end{equation}

\begin{proposition}\label{prop31}
    Se $A$ é uma matriz $n \times n$, então $\det(A) = \det(A^\top)$.
\end{proposition}
\begin{proof}
    De fato, basta usar que $(-1)^\sigma = (-1)^{\sigma^{-1}}$ e temos \begin{align}
        \det(A^\top) &= \sum_{\sigma \in S_n} (-1)^\sigma a^{\sigma(1)}_1 \cdots a^{\sigma(n)}_n \\ &= \sum_{\sigma \in S_n} (-1)^{\sigma^{-1}} a^1_{\sigma^{-1}(1)} \cdots a^n_{\sigma^{-1}(n)} \\ &= \det(A).
    \end{align}
\end{proof}

\begin{theorem}
    Valem as seguintes propriedades:
    \begin{enumerate}
        \item Se $a_j = (a^1_j, \dots, a^n_j)$ e $a^i = (a^i_1, \dots, a^i_n)$, então podemos enxergar $\det$ como uma função das colunas $a_j$ ou das linhas $a^i$. Para toda $\sigma \in S_n$, \begin{equation}
            \det(a_{\sigma(1)}, \dots, a_{\sigma(n)}) = (-1)^\sigma \det(a_1, \dots, a_n) \quad \text{e} \quad \det(a^{\sigma(1)}, \dots, a^{\sigma(n)}) = (-1)^\sigma \det(a^1, \dots, a^n);
        \end{equation}
        \item Se duas colunas ou duas linhas de $A$ forem iguais, então $\det(A) = 0$;
        \item O $\det$ é multilinear, isso é, para todos $i, j$, \begin{equation}
            \det(a_1, \dots, a_{j-1}, \lambda a_j + b_j, a_{j + 1}, \dots, a_n) = \lambda \det(a_1, \dots, a_n) + \det(a_1, \dots, a_{j-1}, b_j, a_{j+1}, \dots, a_n)
        \end{equation} e \begin{equation}
            \det(a^1, \dots, a^{i-1}, \lambda a^i + b^i, a^{i + 1}, \dots, a^n) = \lambda \det(a^1, \dots, a^n) + \det(a^1, \dots, a^{i-1}, b^i, a^{i+1}, \dots, a^n);
        \end{equation}
        \item $\det(\Id_{n \times n}) = 1$;
        \item Se $a_1, \dots, a_n$ ou $a^1, \dots, a^n$ forem linearmente dependentes, então $\det(A) = 0$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Se $\sigma \in S^n$, então \begin{align}
            \det(a_{\sigma(1)}, \dots, a_{\sigma(n)}) &= \sum_{\rho \in S_n} (-1)^\rho a^1_{\rho(\sigma(1))} \cdots a^n_{\rho(\sigma(n))} \\ &= (-1)^\sigma \sum_{\rho \in S_n} (-1)^\rho (-1)^\sigma a^1_{\rho(\sigma(1))} \cdots a^n_{\rho(\sigma(n))} \\ &= (-1)^\sigma \sum_{\rho \in S_n} (-1)^{\rho \circ \sigma} a^1_{\rho(\sigma(1))} \cdots a^n_{\rho(\sigma(n))} \\ &= (-1)^{\sigma} \det(a_1, \dots, a_n).
        \end{align} O resultado para a permutação de linhas sai do fato de que $\det(A) = \det(A^\top)$;

        \item Se $a_i = a_j$, seja $\sigma$ a permutação dada por $\sigma(i) = j$, $\sigma(j) = i$ e que fixa todo o resto. Temos \begin{equation}
            \det(a_1, \dots, a_i, \dots, a_j, \dots, a_n) = -\det(a_1, \dots, a_j, \dots, a_i, \dots, a_n)
        \end{equation} e portanto, como $a_i = a_j$, temos $\det A = -\det A$, assim $\det A = 0$. O resultado para o caso em que duas linhas são iguais sai do fato de que $\det(A) = \det(A^\top)$;

        \item De fato, temos \begin{align}
            \det(a^1, \dots, \lambda a^i + b^i, \dots, a^n) &= \sum_{\sigma \in S_n} (-1)^\sigma a^1_{\sigma(1)} \cdots (\lambda a^i_{\sigma(i)} + b^i_{\sigma(i)}) \cdots a^n_{\sigma(n)} \\ &= \lambda \sum_{\sigma \in S_n} (-1)^\sigma a^1_{\sigma(1)} \cdots a^i_{\sigma(i)} \cdots a^n_{\sigma(n)} + \sum_{\sigma \in S_n} (-1)^\sigma a^1_{\sigma(1)} \cdots b^i_{\sigma(i)} \cdots a^n_{\sigma(n)} \\ &= \lambda \det(a^1, \dots, a^n) + \det(a^1, \dots, b^i, \dots, a^n).
        \end{align} O resultado análogo para o caso da multilinearidade nas colunas sai do fato de que $\det(A) = \det(A^\top)$;

        \item Temos $a^i_j = 0$ se $i \neq j$, e a única prmutação que fixa todos os valores é a identidade, que tem sinal $1$, portannto \begin{equation}
            \det(\Id_{n \times n}) = a^1_1 \cdots a^n_n = 1;
        \end{equation}

        \item Se $a_i = \lambda^1 a_1 + \cdots + \lambda^{i-1}a_{i-1} + \lambda^{i+1}a_{i+1} + \cdots + \lambda^n a_n$, então \begin{align}
            \det(a_1, \dots, a_i, \dots, a_n) &= \det(a_1, \dots, \lambda^1 a_1 + \cdots + \lambda^{i-1}a_{i-1} + \lambda^{i+1}a_{i+1} + \cdots + \lambda^n a_n, \dots, a_n) \\ &= \sum_{j = 1, j \neq i}^n \lambda^j \det(a_1, \dots, a_j, \dots, a_n)
        \end{align} e, como cada $a_j$ é uma cópia de outra coluna, todos os determinantes dentro do somatório são nulos, assim $\det(A) = 0$. O resultado análogo para a dependência linear das linhas sai do fato de que $\det(A) = \det(A^\top)$.
    \end{enumerate}
\end{proof}

Essas propriedades são importantes para fazer contas, porém, elas são mais importantes ainda pois podem ser utilizadas para definir o determinante!

\begin{proposition}
    As propriedades $1$, $3$ e $4$ definem unicamente o determinante.
\end{proposition}
\begin{proof}
    Queremos mostrar que qualquer função $D(a_1, \dots, a_n)$ que seja multilinear, alternada e que tenha valor $1$ na base canônica de $\mathbb{R}^n$, é o determinante. Note que as propriedades $2$ e $5$ também valém para qualquer $D$ desse tipo, já que são consequências das outras.

    Como $a_j = (a^1_j, \dots, a^n_j) = a^1e_i + \cdots + a^ne_n$ onde $e_i$ é a base canônica, então usando as propriedades $3$ e $4$ temos \begin{align}
        D(a_1, \dots, a_n) &= \sum_{i_1 = 1}^n a^{i_1}_1 D(e_{i_1}, a_2, \dots, a_n) \\ &= \sum_{i_1 = 1}^n \sum_{i_2 = 1}^n a^{i_1}_1 a^{i_2}_2 D(e_{i_1}, e_{i_2}, a_3, \dots, a_n) \\ &= \sum_{i_1 = 1}^n \cdots \sum_{i_n = 1}^n a^{i_1}_1 \cdots a^{i_n}_n D(e_{i_1}, \dots, e_{i_n})
    \end{align} e, denotando por $\sigma_{i_1, \dots, i_n}$ a permutação $\sigma(j) = i_j$ temos \begin{equation}
        D(e_{i_1}, \dots, e_{i_n}) = (-1)^{\sigma_{i_1, \dots, i_n}} D(e_1, \dots, e_n) = (-1)^{\sigma_{i_1, \dots, i_n}}.
    \end{equation} Porém, como todas as combinações de $i_1, \dots, i_n$ são atingidas nos somatórios, $\sigma_{i_1, \dots, i_n}$ eventualmente se passa por todas as permutações, assim \begin{equation}
        D(a_1, \dots, a_n) = \sum_{\sigma \in S_n} (-1)^\sigma a^{\sigma(1)}_1 \cdots a^{\sigma(n)}_n = \det(A^\top) = \det(A).
    \end{equation}
\end{proof}

O próximo passo é mostrar que o determinante é multiplicativo. Para isso introduzimos uma notação. Se $A$ é uma matriz $n \times n$ e $t \in \mathbb{R}$, então $A + t$ é definido como $A + t\Id_{n \times n}$.

\begin{theorem}
    Se $A$ e $B$ são matrizes $n \times n$, então $\det(AB) = \det(A)\det(B)$.
\end{theorem}
\begin{proof}
    Suponha que $\det(B) \neq 0$ e defina $D(A) = \det(AB)/\det(B)$. Seja $e_1, \dots, e_n$ é a base canônica de $\mathbb{R}^n$. Se $f_i \in \mathbb{R}^n$, então existe uma matriz $n \times n$ tal que $f_i = Ce_i$. Assim, \begin{align}
        D(Ae_1, \dots, \lambda Ae_i + Ce_i, \dots, Ae_n) &= \frac{\det(ABe_1, \dots, (\lambda A+C)Be_i, \dots, ABe_n)}{\det(B)} \\ &= \frac{\lambda \det(ABe_1, \dots, ABe_n) + \det(ABe_1, \dots, CBe_i, \dots, ABe_n)}{\det(B)} \\ &= \lambda D(Ae_1, \dots, Ae_n) + D(Ae_1, \dots, Ce_i, \dots, Ae_n),
    \end{align} da onde segue a multilinearidade. Mais ainda, \begin{equation}
        D(\Id_{n \times n}) = \det(\Id_{n \times n} B)/\det(B) = \det(B)/\det(B) = 1
    \end{equation} e \begin{align}
        D(Ae_{\sigma(1)}, \dots, Ae_{\sigma(n)}) &= \det(AB_{\sigma(1)}, \dots, ABe_{\sigma(n)})/\det(B) \\ &= (-1)^\sigma \det(ABe_1, \dots, ABe_n)/\det(B) \\ &= (-1)^\sigma D(Ae_1, \dots, Ae_n),
    \end{align} portanto segue que $D(A) = \det(A)$ e assim $\det(AB) = \det(A)\det(B)$. Se $\det(B) = 0$, tome $B(t) = B + t$. Temos que $\det(B(t))$ é um polinômio não nulo (afinal, é mônico), portanto $\det(AB(t)) = \det(A)\det(B(t))$ e, tomando $t \to 0$, o resultado segue.
\end{proof}

\begin{corollary}
    Uma matriz quadrada $A$ é invertível se, e somente se, $\det(A) \neq 0$.
\end{corollary}
\begin{proof}
    Se $A$ é invertível, então existe $B$ com $AB = \Id$, assim $\det(A) \det(B) = \det(\Id) = 1$, portanto é impossível que $\det(A)$ seja nulo. Por outro lado, se $A$ não é invertível, então $A$ não é sobrejetora, portanto sua imagem, o espaço gerado pelas suas colunas, é próprio, da onde segue que suas colunas são linearmente dependentes e assim o seu determinante é nulo.
\end{proof}

Deixo aqui mais duas fórmulas que podem ser úteis para o cálculo de determinantes e de inversas, sem as demonstrações. se $A$ é uma matriz $n \times n$, denotamos por $A^i_j$ a matriz $(n-1) \times (n-1)$ dada pela remoção da $i$-ésima linha e da $j$-ésima coluna de $A$. Valem as seguintes identidades: \begin{equation}
    \det(A) = \sum_{i = 1}^n (-1)^{i + j} a^i_j \det(A^i_j) \quad \text{e} \quad A^{-1} = \left[(-1)^{i + j}\frac{\det(A_{ji})}{\det(A)}\right]_{n \times n}.
\end{equation}

Para terminarmos esse capítulo, vamos falar do traço. Se $A$ é uma matriz $n \times n$, seu \textbf{traço} é definido por \begin{equation}
    \tr(A) = \sum_{i = 1}^n a^i_i.
\end{equation} Fica claro da definição que o traço é linear.

\begin{proposition}
    Se $A$ e $B$ são matrizes $n \times n$, $\tr(AB) = \tr(BA)$.
\end{proposition}
\begin{proof}
    Temos \begin{equation}
        \tr(AB) = \sum_{i = 1}^n \sum_{k = 1}^n a^i_k b^k_i = \sum_{k = 1}^n \sum_{i = 1}^n a^k_i b^i_k = \tr(BA)
    \end{equation}
\end{proof}

\begin{proposition}
    Se $A$ é uma matriz $n \times n$, então \begin{equation}
        \tr(AA^\top) = \sum_{i = 1}^n \sum_{j = 1}^n (a^i_j)^2
    \end{equation}
\end{proposition}
\begin{proof}
    De fato, \begin{equation}
        \tr(AA^\top) = \sum_{i = 1}^n \sum_{j = 1}^n a^i_j a^i_j = \sum_{i = 1}^n \sum_{j = 1}^n (a^i_j)^2
    \end{equation}
\end{proof}

Agora recordamos a noção de similaridade, mas para matrizes. Se $M$ e $T$ são matrizes $n \times n$, dizemos que $M$ é \textbf{similar} a $T$ se $M = STS^{-1}$ para alguma matriz $S$ invertível.

\begin{proposition}
    Matrizes similares possuem mesmo traço e determinante.
\end{proposition}
\begin{proof}
    Se $M = STS^{-1}$, então \begin{align}
        \det(M) = \det(STS^{-1}) = \det(S^{-1}ST) = \det(T) \quad \text{e} \quad \tr(M) = \tr(STS^{-1}) = \tr(S^{-1}ST) = \tr(T).
    \end{align}
\end{proof}

O próximo passo agora é falar sobre mudanças de coordenadas. Se $e = (e_1, \dots, e_n)$ e $f = (f_1, \dots, f_n)$ são bases de $V$, então podemos escrever \begin{equation}
    e_j = \sum_{i = 1}^n a^i_j f_i.
\end{equation} A matriz $A = [a^i_j]_{n \times n}$ leva a base $f$ na base $e$ (escrevemos $e = Af$). Note que $A$ é invertível, afinal, caso seu determinante fosse nulo, os vetores $e_j$ seriam linearmente dependentes. Agora, se $v[e] = (v^1, \dots, v^n)$ e $v[f] = (u^1, \dots, u^n)$, então \begin{equation}
    v = \sum_{j = 1}^n v^j e_j = \sum_{j = 1}^n v^j \sum_{i = 1}^n a^i_j f_i = \sum_{i = 1}^n \left(\sum_{j = 1}^n a^i_j v^j\right) f_i,
\end{equation} portanto $v[f] = Av[e]$ e assim $v[e] = A^{-1}v[f]$. A matriz $A$ portanto é chamada de \textbf{matriz de mudança da base $e$ para a base $f$}. Note que a mesma matriz que leva a base $f$ na base $e$, leva as coordenadas de $v$ na base $e$ para as coordenadas de $v$ na base $f$, isso é: \begin{quotation}
    ``as coordenadas de um vetor mudam contra a mudança de base''.
\end{quotation} Por esse motivo, dizemos que vetores são \textbf{quantidades contravariantes}. Por outro lado, considerando as bases duais $\varepsilon$ e $\phi$ de $e$ e $f$, se $\mu[\varepsilon] = (\mu_1, \dots, \mu_n)$ então \begin{equation}
    \mu(v) = \sum_{i = 1}^n \mu_i \phi^i(v) = \sum_{i = 1}^n \mu_i u^i = \sum_{i = 1}^n \mu_i \sum_{j = 1}^n a^i_j v^j = \sum_{j = 1}^n \left(\sum_{i = 1}^n a^i_j \mu_i\right)\varepsilon^j(v),
\end{equation} da onde segue que $\mu[e] = A\mu[f]$, ou seja, \begin{quotation}
    ``as coordenadas de um covetor mudam a favor da mudança de base''.
\end{quotation} Por esse motivo, dizemos que vetores são \textbf{quantidades covariantes}. Índices em quantidades contravariantes sempre são colocados embaixo, e os índices em quantidades covariantes sempre são colocados em cima. Índices em coordenadas sempre são colocados ao contrário (coordenadas de vetores tem índice em cima, e coordenadas de vetores tem índice em baixo).

Se $T \colon V \to W$ é linear, então fixe $e$ e $f$ bases de $V$ e $x$ e $y$ bases de $W$. Vamos entender como se da a transformação de $[T]_{f,y}$ em $[T]_{e,x}$. Se \begin{equation}
    Te_j = \sum_{i = 1}^m \lambda^i_j x_i \quad \text{e} \quad Tf_j = \sum_{i = 1}^m \mu^i_j y_i,
\end{equation} então seja $A = [a^i_j]_{n \times n}$ a matriz de mudança de base de $e$ para $f$ e $B = [b^i_j]_{m \times m}$ a matriz de mudança de base de $y$ para $x$, então \begin{equation}
    Tf_i = \sum_{r = 1}^n \mu^r_i y_r = \sum_{k = 1}^n \sum_{r = 1}^n \mu^r_i b^k_r x_k
\end{equation} e assim \begin{equation}
    Te_j = \sum_{i = 1}^n a^i_j Tf_i = \sum_{k = 1}^n \sum_{r = 1}^n \sum_{i = 1}^n b^k_r \mu^r_i a^i_j x_k.
\end{equation} Isso mostra que $[T]_{e,x} = B[T]_{f,y}A$. Tomando $W = V$, $x = e$ e $y = f$, então $B = A^{-1}$, assim $[T]_{e,e}$ é similar a $[T]_{f,f}$. Portanto, podemos definir o \textbf{determinante} de $T$ como sendo o determinante da matriz de $T$ em alguma base, e o conceito está bem definido, pois matrizes similares possuem o mesmo determinante. O mesmo vale para o traço.

\subsection{Estrutura Euclidiana e Formas Bilineares}

A partir de agora, tomamos $\mathbb{K} = \mathbb{R}$, ou seja, todo escalar é real. Uma \textbf{estrutura Euclidiana} em um espaço vetorial $V$ é um mapa \begin{equation}
    \begin{split}
        \langle \cdot, \cdot \rangle \colon V \times V &\to \mathbb{R} \\ (v,u) &\mapsto \langle v, u \rangle
    \end{split}
\end{equation} que satisfaz: \begin{itemize}
    \item para todo $v \in V$ com $v \neq 0$, $\langle v, v \rangle > 0$ - \textbf{positividade};
    \item para todos $u, v \in V$, $\langle v, u \rangle = \langle u, v \rangle$ - \textbf{simetria};
    \item para todos $u, v, w \in V$ e $\lambda \in \mathbb{R}$, $\langle \lambda v + u, w \rangle = \lambda \langle v, w \rangle + \langle u, w \rangle$ - \textbf{linearidade} na primeira entrada, que junto com a simetria se torna \textbf{bilinearidade}.
\end{itemize}
Um mapa dessa forma é chamado de \textbf{produto interno}, e um espaço com uma estrutura Euclidiana é um \textbf{espaço Euclidiano}. A \textbf{norma} de um vetor $v$ é o número $||v|| = \sqrt{\langle v, v \rangle}$. A norma representa a distância de $v$ a $0$, portanto a \textbf{distância} entre $u$ e $v$ é definida por $||u - v||$. Os próximos dois teoremas são chamados de desigualdade de Cauchy-Schwarz e desigualdade triangular.

\begin{theorem}
    Dados $u, v \in V$, temos $|\langle v, u \rangle| \leq ||v||\cdot||u||$.
\end{theorem}
\begin{proof}
    Se $u = 0$, então a desigualdade é trivialmente verdadeira. Considere o mapa $q(t) = ||v + tu||^2$. Usando a bilinearidade, temos que \begin{equation}
        q(t) = ||v||^2 + 2t\langle v, u \rangle + t^2||u||^2.
    \end{equation} Tome $t = -\langle v, u \rangle/||u||^2$ e temos \begin{equation}
        q(t) = ||v||^2 - \frac{\langle v, u \rangle^2}{||u||^2} \geq 0,
    \end{equation} da onde segue o resultado.
\end{proof}

\begin{theorem}
    Dados $u, v \in V$, temos $||v + u|| \leq ||v|| + ||u||$.
\end{theorem}
\begin{proof}
    Temos \begin{equation}
        ||v + u||^2 = ||v||^2 + 2\langle v, u \rangle + ||u||^2 \leq ||v||^2 + 2||v||\cdot||u|| + ||u||^2 = (||v|| + ||u||)^2.
    \end{equation} Tirando a raíz dos dois lados, a desigualdade segue.
\end{proof}

Dois vetores $u$ e $v$ são \textbf{perpendiculares} ou \textbf{ortogonais} se $\langle v, u \rangle = 0$. Fica claro que, nesse caso, vale o teorema de pitágoras: $||v - u||^2 = ||v||^2 + ||u||^2$. Se $e_1, \dots, e_n$ é uma base de $V$, dizemos que ela é \textbf{ortonormmal} se \begin{equation}
    \langle e_i, e_j \rangle = \delta^i_j = \begin{cases}
        0, &\text{se } i \neq j, \\ 1, &\text{se } i = j.
    \end{cases}
\end{equation} Note que $||e_i|| = \sqrt{\langle e_i, e_i \rangle} = 1$. O principal resultado é que toda base pode ser transformada numa base ortonormal.

\begin{theorem}
    O teorema a seguir é chamado de ortonormalização de Gram-Schmidt. Dada uma base $f_1, \dots, f_n$ de $V$, existe uma outra base $e_1, \dots, e_n$ com as seguintes propriedades: \begin{enumerate}
        \item $e_1, \dots, e_n$ é ortonormal;
        \item $e_k$ é uma combinação linear de $f_1, \dots, f_k$ para todo $k$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    Definimos $e_1 = f_1/||f_1||$. Se $e_1, \dots, e_{k-1}$ já estiverem definidos, definimos \begin{equation}
        e_k = c\left(f_k - \sum_{j = 1}^{k-1} c_j e_j\right)
    \end{equation} com $c_j = \langle f_k, e_j \rangle$ e $c$ escolhido de tal forma que $||e_k|| = 1$. Se $l < k$, então \begin{equation}
        \langle e_k, e_l \rangle = c\langle f_k, e_l \rangle - c\sum_{j = 1}^{k - 1} \langle f_k, e_j \rangle \langle e_j, e_l \rangle = (c - c)\langle f_k, e_l \rangle = 0.
    \end{equation} O caso $l > k$ é consequência do caso $l < k$ por simetria.
\end{proof}

Se $e = (e_1, \dots, e_n)$ é uma base ortonormal, $x[e] = (x^1, \dots, x^n)$ e $y[e] = (y^1, \dots, y^n)$, então fica claro que \begin{equation}
    \langle x, y \rangle = \sum_{i = 1}^n x^i y^i \quad \text{e} \quad ||x||^2 = \sum_{i = 1}^n (x^i)^2.
\end{equation} O resultado que segue é chamado de teorema de representação de Riesz (versão de dimensão finita).

\begin{theorem}
    Dado $\phi \colon V \to \mathbb{R}$ um covetor, existe um $v \in V$ tal que $\phi(u) = \langle u, v \rangle$. A associação $\phi \mapsto u$ é um isomorfismo linear entre $V$ e $V^*$.
\end{theorem}
\begin{proof}
    Seja $e = (e_1, \dots, e_n)$ uma base ortonormal. Se $v[e] = (v^1, \dots, v^n)$, então \begin{equation}
        \phi(u) = \sum_{i = 1}^n v^i \phi(e_i) = \sum_{i = 1}^n \langle v, e_i \rangle \phi(e_i) = \left\langle v, \sum_{i = 1}^n \phi(e_i) e_i \right\rangle.
    \end{equation} Como o segundo vetor não depende de $v$, fica provada a primeira parte do resultado. Se $\phi(v) = \langle v, u \rangle$ e $\iota(v) = \langle v, w \rangle$, então \begin{equation}
        (\phi + \iota)(v) = \langle v, u \rangle + \langle v, w \rangle = \langle v, u + w \rangle \quad \text{e} \quad (\lambda \phi)(v) = \lambda \langle v, u \rangle = \langle v, \lambda u \rangle,
    \end{equation} assim a associação $\phi \mapsto u$ é linear. Se $u = 0$, então claramente $\phi = 0$, assim o mapa é injetor e, como $\dim V = \dim V^*$, é um isomorfismo.
\end{proof}

Dado um subespaço $U$ de $V$, o \textbf{complemento ortogonal} de $U$ em $V$ é o subespaço \begin{equation}
    U^\perp = \{v \in V \mid \langle v, u \rangle = 0, \text{ para todo } u \in U\}.
\end{equation} Essa notação pode ser ambígua com a notação para o aniquilador de $U$, mas isso é por que eles são o mesmo espaço, através da representação de Riesz.

\begin{proposition}
    Se $U$ é um subespaço de $V$, então $U \oplus U^\perp = V$.
\end{proposition}
\begin{proof}
    Se $U'$ é o aniquilador, sabemos que $\dim U' = \dim U^\perp$, assim $\dim U^\perp + \dim U = \dim V$, portanto $\dim U \cap U^\perp = 0$, assim a soma é direta e o resultado está provado.
\end{proof}

Pelo resultado anterior, cada $v \in V$ se quebra unicamente em $v = x + y$ com $x \in U$ e $y \in U^\perp$. A \textbf{projeção} em $U$ é o mapa $Pv = x$.

\begin{proposition}
    O mapa $P$ é linear e $P^2 = P$.
\end{proposition}
\begin{proof}
    Se $v = x + y$ e $w = r + s$ com $x, r \in U$ e $y, s \in U^\perp$, então $v + w = (x + r) + (y + s)$ e, pela unicidade da decomposição, $P(v + w) = x + r = Pv + Pw$. Além disso, $\lambda v = \lambda x + \lambda y$ e, pela unicidade da decomposição $P(\lambda v) = \lambda x = \lambda Pv$. Por fim, $P^2v = P(Pv) = Px = x$, pois $x \in U$, assim $P^2 = P$.
\end{proof}

\begin{theorem}
    O vetor $Pv$ minimiza, em $U$, a distância até $x$.
\end{theorem}
\begin{proof}
    Se $v = x + y$ com $x \in U$ e $y \in U^\perp$, dado $w \in U$ temos \begin{equation}
        v - w = x - w + y
    \end{equation} e, como $x - w \in U$, então $||v - w||^2 = ||x - w||^2 + ||y||^2$. Claramente o valor $||v - w||^2$ é mínimo quando $w = x$.
\end{proof}

Se $T \colon V \to W$ é um mapa entre espaços Euclidianos, então considere o transposto $T^\top \colon W^* \to V^*$. Fazendo a identificação de $W^*$ e $V^*$ com $W$ e $V$, temos um mapa $T^* \colon W \to V$, chamado de \textbf{adjunto} de $T$.

\begin{proposition}
    Se $v \in V$, $w \in W$ e $T \colon V \to W$ é linear, então $\langle Tv, w \rangle = \langle v, T^*w \rangle$. De fato, se $S \colon W \to V$ é linear e satisfaz $\langle Tv, w \rangle = \langle v, Sw \rangle$, então $S = T^*$.
\end{proposition}
\begin{proof}
    Denotamos $\phi_x(v) = \langle v, x \rangle$. Se $u = T^*w$, então $\phi_u = T^\top(\phi_w)$, assim \begin{equation}
        \langle v, T^*w \rangle = \phi_u(v) = T^\top(\phi_w)v = (\phi_w \circ T)v = \phi_w(Tv) = \langle Tv, w \rangle.
    \end{equation}
    Por fim, se $S$ satisfaz a igualdade, então se $u = Sw$, temos $\phi_u(v) = \phi_w(Tv) = T^\top(\phi_w)v$, assim $\phi_u = T^\top(\phi_w)$ da onde segue que $S = T^*$.
\end{proof}

O próximo passo é demonstrar algumas propriedades sobre o adjunto.

\begin{proposition}
    Se $T, S \colon V \to W$ e $R \colon W \to U$ são lineares, então \begin{equation}
        (T + S)^* = T^* + S^*,, \quad (T^*)^* = T \quad \text{e} \quad (RT)^* = T^* R^*.
    \end{equation} Além disso, se $T$ for um isomorfismo, então $(T^{-1})^* = (T^*)^{-1}$.
\end{proposition}
\begin{proof}
    Note que \begin{equation}
        \langle (T+S)v, u \rangle = \langle Tv, u \rangle + \langle Sv, u \rangle = \langle v, T^*u \rangle + \langle v, S^*u \rangle = \langle v, (T^* + S^*)u \rangle.
    \end{equation} Além disso, \begin{equation}
        \langle T^*u, v \rangle = \langle v, T^*u \rangle = \langle Tv, u \rangle = \langle u, Tv \rangle
    \end{equation} e também temos \begin{equation}
        \langle RTv, u \rangle = \langle Tv R^*u \rangle = \langle v, T^*R^*u \rangle.
    \end{equation} Por fim, \begin{equation}
        TT^{-1} = \Id_W \implies (T^{-1})^*T^* = \Id_W,
    \end{equation} concluindo o resultado desejado.
\end{proof}

Vamos agora achar a matriz da transformação de Riesz. Se $e$ é uma base ortonormal de $V$ e $\varepsilon$ é sua base dual, então se $R_V$ é a representação de Riesz, temos que \begin{eqnarray}
    R_V\varepsilon^j =  e_j
\end{eqnarray} assim a matriz de $R_V$ (com respeito a essas bases) é a identidade! Segue que a matriz do mapa adjunto, com respeito a duas bases ortogonais, assim como do mapa transposto, é a matriz transposta do mapa original.

Se $V$ e $W$ são espaços Euclidianos, um mapa $T \colon V \to W$ \textbf{preserva distâncias} se \begin{eqnarray}
    ||Tv - Tu|| = ||v - u||
\end{eqnarray} para todos $v, u \in V$. Mapas que preserva distâncias são sempre injetores. Um mapa bijetor que preserva distâncias é uma \textbf{isometria} entre $V$ e $W$, e nesse caso dizemos que $V$ e $W$ são \textbf{isométricos}.

\subsection{Teoria Espectral}

\section{Grupos}
\section{Anéis}
\section{Corpos}
\section{Métricos}
\section{Análise 1}
\section{Análise 2}
\section{Análise Complexa}
\section{Medida}
\section{Funcional}
\section{EDO}
\section{EDP}
\section{Probabilidade}
\section{Topologia}
\section{Topologia Algébrica}
\section{Topologia Diferencial}
\section{Análise em Variedades}
\section{Riemanniana}

\end{document}