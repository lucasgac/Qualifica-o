\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6.5in, 9.5in}]{geometry}
\usepackage[portuguese]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{babel,arrows,positioning,chains,matrix,scopes,cd,quotes,calc,decorations.pathmorphing}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithmic}
\usepackage[portuguese, ruled, lined]{algorithm2e}
\usepackage{setspace}
\usepackage[T1]{fontenc}
\usepackage{csquotes}
\usepackage{pythonhighlight}
\usepackage{algorithmic}
\usepackage[portuguese, ruled, lined]{algorithm2e}

%quiver
\tikzset{curve/.style={settings={#1},to path={(\tikztostart)
    .. controls ($(\tikztostart)!\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    and ($(\tikztostart)!1-\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    .. (\tikztotarget)\tikztonodes}},
    settings/.code={\tikzset{quiver/.cd,#1}
        \def\pv##1{\pgfkeysvalueof{/tikz/quiver/##1}}},
    quiver/.cd,pos/.initial=0.35,height/.initial=0}

% TikZ arrowhead/tail styles.
\tikzset{tail reversed/.code={\pgfsetarrowsstart{tikzcd to}}}
\tikzset{2tail/.code={\pgfsetarrowsstart{Implies[reversed]}}}
\tikzset{2tail reversed/.code={\pgfsetarrowsstart{Implies}}}
% TikZ arrow styles.
\tikzset{no body/.style={/tikz/dash pattern=on 0 off 1mm}}

\newtheorem{definition}{Definição}
\newtheorem{proposition}[definition]{Proposição}
\newtheorem{lemma}[definition]{Lema}
\newtheorem{axiom}[definition]{Axioma}
\newtheorem{corollary}[definition]{Corolário}
\newtheorem{theorem}[definition]{Teorema}
\newtheorem{distribution}[definition]{Distribuição}
\newtheorem{example}[definition]{Exemplo}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}

\DeclareMathOperator{\freeab}{Free_{Ab}}
\DeclareMathOperator{\Top}{Top}
\DeclareMathOperator{\Ab}{Ab}
\DeclareMathOperator{\Grp}{Grp}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\module}{Mod}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\chain}{Ch}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Alt}{Alt}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\Ricci}{Ricci}
\DeclareMathOperator{\arccosh}{arccosh}
\DeclareMathOperator{\gyr}{gyr}
\DeclareMathOperator{\arctanh}{arctanh}
\DeclareMathOperator{\Log}{Log}
\DeclareMathOperator{\ArithmeticMod}{mod}

\renewcommand{\phi}{\varphi}

\newcommand{\modulus}[1]{\phantom{.}(\ArithmeticMod\phantom{.}#1)}

\newcommand{\Mod}[1]{$\module_{#1}$}
\newcommand{\Chain}[1]{$\chain(#1)$}

\newcommand{\openset}[0]{{\phantom{}\subset}{\circ}\phantom{.}}

\def\arrvline{\hfil\kern\arraycolsep\vline\kern-\arraycolsep\hfilneg}

\title{Qualificação de Mestrado}
\author{Lucas Giraldi Almeida Coimbra}

\begin{document}

\maketitle
\tableofcontents

\section{Álgebra Linear}

\subsection{Fundamentos e Dualidade}

Tome $\mathbb{K} = \mathbb{R}$ ou $\mathbb{C}$. Um \textbf{espaço vetorial} é um conjunto $V$ munido de duas operações \begin{equation}
    \begin{split}
        + \colon V \times V &\to V \\ (x,y) &\mapsto x + y
    \end{split} \quad \text{e} \quad \begin{split}
        \cdot \colon \mathbb{K} \times V &\to V \\ (\lambda,x) &\mapsto \lambda x
    \end{split}
\end{equation} tais que a operação $+$ (soma) é comutativa, associativa, possui identidade e todos os inversos, e a operação de $\cdot$ (produto por escalar) satisfaz as relações distributivas, $1x = x$ e $\lambda(\mu x) = (\lambda\mu)x$.

Um \textbf{subespaço vetorial} de um espaço vetorial $V$ é um subconjunto $S \subset V$ tal que para todos $x, y \in S$ e $\lambda \in \mathbb{K}$, temos $x + y \in S$ e $\lambda x \in S$. Todo espaço vetorial é um subespaço vetorial de si mesmo, assim como $\{0\}$ é sempre um subespaço vetorial. Chamamos $V$ e $\{0\}$ de \textbf{subespaços triviais}. Se $U, W \subset V$ são dois subespaços, então o conjunto $U + V = \{u + w \mid u \in U, w \in W\}$ é a \textbf{soma} desses subespaços, e é também um subespaço. Se $U \cap W = \{0\}$, então diremos que a soma desses espaços é uma \textbf{soma direta} e a denotamos por $U \oplus W$. A intersecção $U \cap W$ também é sempre um subespaço vetorial.

Uma \textbf{combinação linear} de vetores em $V$ é uma soma finita \begin{equation}
    \lambda^i x_i = 0
\end{equation} onde cada $\lambda^i \in \mathbb{K}$. Dado um conjunto $S \subset V$, o conjunto $\langle S \rangle$ de todas as combinações lineares de elementos de $S$ é um subespaço vetorial de $V$, chamado de \textbf{subespaço gerado por $S$}. O conjunto $S$ é \textbf{gerador} de $V$ se $\langle S \rangle = V$.

Dizemos que $x_1, \dots, x_n \in V$ são \textbf{linearmente independentes} se para quaisquer $\lambda^1, \dots, \lambda^n \in \mathbb{K}$ tais que \begin{equation}
    \lambda^i x_i = 0,
\end{equation} então $\lambda_i = 0$ para todo $i$. Vetores que não são linearmente independentes são \textbf{linearmente dependentes}. Fica claro da definição que um conjunto de vetores é linearmente dependente se, e somente se, um dos vetores pode ser escrito como combinação linear dos outros. Além disso, é fácil ver que se uma subcoleção de vetores é linearmente dependente, então a coleção original também é. Mais ainda, qualquer coleção de vetores que contenha o $0$ é linearmente dependente.

\begin{lemma}\label{lemma1}
    Sejam $S = \{s_1, \dots, s_n\}$ um gerador de $V$ e $v_1, \dots, v_m$ vetores linearmente independentes. Então, $m \leq n$.
\end{lemma}
\begin{proof}
    Suponha que $m > n$. Como $S$ gera $V$, então existem $\lambda^1, \dots, \lambda^n$ tais que $y_1 = \lambda^i s_i$. Como $y_1 \neq 0$ (pela independência linear), então algum $\lambda_j$ é não nulo, ou seja, podemos substituir $s_j$ por $y_1$ e o conjunto resultante ainda gera $V$. Pela independência linear dos $y_i$, podemos fazer essa operação mais $n-1$ vezes, garantindo que $y_1, \dots, y_n$ geram $V$. Porém, isso significa que $y_{n+1}, \dots, y_m$ são combinação linear de $y_1, \dots, y_n$, o que contradiz a independência linear. Segue então que $m \leq n$.
\end{proof}

Um espaço $V$ é \textbf{finitamente gerado} se existe um conjunto gerador finito. Uma \textbf{base} de $V$ é um conjunto gerador linearmente independente.

\begin{lemma}\label{lemma2}
    Todo espaço finitamente gerado possui uma base.
\end{lemma}
\begin{proof}
    Se $S = \{s_1, \dots, s_n\}$ gera $V$, então se $S$ é linearmente independente o trabalho acabou. Caso contrário, algum $s_i$ é combinação linear dos outros, e então retiramos ele e o conjunto resultante ainda gera $V$. Fazemos isso até que os vetores que sobram em $S$ sejam linearmente independentes, e assim temos uma base.
\end{proof}

A partir de agora vamos trabalhar apenas com espaços finitamente gerados e, caso queiramos falar em um contexto mais geral, iremos explicitar. A \textbf{dimensão} de um espaço $V$, denotada por $\dim V$, é o número de elementos de uma base. Pelo Teorema a seguir, esse número está bem definido.

\begin{theorem}
    Toda base possui mesmo número de elementos.
\end{theorem}
\begin{proof}
    Como bases são linearmente independentes e geradoras, o resultado segue facilmente do Lema \ref{lemma1}.
\end{proof}

O \ref{lemma2} assume implicitamente que o conjunto $S$ que gera $V$ é não vazio. Caso tenhamos $V = \langle\varnothing\rangle$, então $V = \{0\}$ e o chamamos de \textbf{espaço trivial}. Sua dimensão é, por definição, nula.

\begin{theorem}
    Todo conjunto linearmente independente pode ser estendido para uma base.
\end{theorem}
\begin{proof}
    Se $S$ é um conjunto linearmente independente, então considere $\langle S \rangle$. Se $\langle S \rangle = V$, então o conjunto $S$ já é uma base. Caso contrário, seja $v \in V \setminus \langle S \rangle$ e tome $S_1 = S \cup \{v\}$. Podemos agora testar se $\langle S_1 \rangle = V$ e, caso contrário, repetir o processo. Como o espaço $V$ é finitamente gerado, esse processo obrigatoriamente acaba, que é quando adicionamos vetores o suficiente em $S$ para que se torne uma base.
\end{proof}

Note que todo subespaço de um espaço com dimensão finita, possui dimensão finita (pelo Lema \ref{lemma1}). Se $W$ é um subespaço de $V$, um subespaço $U$ de $V$ é um \textbf{complemento} de $W$ se $U \oplus W = V$.

\begin{theorem}
    Complementos sempre existem e são únicos.
\end{theorem}
\begin{proof}
    Se $W$ é um subespaço, seja $v_1, \dots, v_m$ uma base de $W$ e a complete para uma base $v_1, \dots, v_m, \\w_1, \dots, w_n$ de $V$. Defina $U = \langle w_1, \dots, w_n \rangle$. Se $x \in U \cap W$, então existem $\lambda^1, \dots, \lambda^m$ e $\mu^1, \dots, \mu^n$ em $\mathbb{K}$ tais que \begin{equation}
        x = \lambda^i v_i = \mu^j w_j,
    \end{equation}
    ou seja, $\lambda^i v_i + \mu^j v_j = 0$, portanto cada $\lambda^i$ e $\mu^j$ é nulo, da onde segue que $x = 0$ e assim $U \cap W = \{0\}$. Mais ainda, se $x \in V$, então podemos escrever $x = \lambda^i v_i + \mu^j w_j$ e assim $x = v + w$ com $v \in U$ e $w \in W$, da onde segue que $V = U \oplus W$.
\end{proof}

Note que da demonstração acima tiramos um outro fato importante: se $V = U \oplus W$, então $\dim V = \dim U + \dim W$. Esse fato pode ser generalizado, isso é, se $V = V_1 + \cdots + V_n$ e $V_i \cap V_j = \{0\}$ quando $i \neq j$, então escrevemos \begin{equation}
    V = V_1 \oplus \cdots \oplus V_n = \bigoplus_{i = 1}^n V_i
\end{equation} e nesse caso temos \begin{equation}
    \dim V = \sum_{i = 1}^n \dim V_i.
\end{equation}

\begin{proposition}
    Se $V = V_1 \oplus \cdots \oplus V_n$ e $x \in V$, então existem $x_i \in V_i$ únicos tais que $x = x_1 + \cdots + x_n$.
\end{proposition}
\begin{proof}
    Como já observamos, se $v_i^j$ é uma base de $V_j$, então podemos escrever $x = \lambda_j^i v_i^j$ e assim tomamos $x_j = \lambda^i v_i^j$
\end{proof}

----------------------------

Dadas duas bases $e = (e_1, \dots, e_n)$ e $f = (f_1, \dots, f_n)$ de $V$, podemos escrever unicamente cada $e_j$ como \begin{equation}\label{eq10}
    e_j = a^i_j f_i
\end{equation} e fica claro que o determinante da matriz $A = [a^i_j]$ é não nulo, caso contrário os vetores $e_j$ seriam linearmente dependentes. Se $x[e] = (\lambda^1, \dots, \lambda^n)$ e $x[f] = (\mu^1, \dots, \mu^n)$, então sabemos que $\lambda^j e_j = \mu^i f_i$ e portanto \begin{equation}
    \mu^i f_i = \lambda^j a^i_j f_i.
\end{equation} Como os vetores $f_i$ são linearmente independentes, segue que $\mu^i = a^i_j \lambda^j$. Dessa forma, temos que $x[e] = A x[f]$, portanto $x[f] = A^{-1} x[e]$, ou seja, \textit{a troca de coordenadas de $x$ da base $e$ para base $f$ é dada pela matriz inversa da matriz que troca a base $f$ para a base $e$} (como vimos em \ref{eq10}). Podemos abreviar essa frase dizendo que vetores são \textbf{quantidades contravariantes}, no sentido de que eles mudam de coordenadas de maneira inversa a uma mudança de base. Em particular, é por isso que sempre denotamos os índices de vetores embaixo. Para \textbf{quantidades covariantes}, os índices são denotados em cima (números não obedecem essa regra e seus índices são posicionados de maneira a obedecer a notação de soma de Einstein).

\section{Grupos}
\section{Anéis}
\section{Corpos}
\section{Métricos}
\section{Análise 1}
\section{Análise 2}
\section{Análise Complexa}
\section{Medida}
\section{Funcional}
\section{EDO}
\section{EDP}
\section{Probabilidade}
\section{Topologia}
\section{Topologia Algébrica}
\section{Topologia Diferencial}
\section{Análise em Variedades}
\section{Riemanniana}

\end{document}